Data -> Process -> Information
3V: volume, velocity, variety aspects.

Spatial Data:
    -Very tied to the physical space.
    -Multi-dimensional.
    -Complex geometrical shapes.
    -New query API.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Database:
    -contains many tables. A very large, integrated collection of data. Models real-world enterprise(Entities and Relationships).
    -is stored as a collection of files. Each file is a sequence of records. A record is a sequence of fields.
     one file   <---> one table
     one record <---> one tuple

Database Management System:
    -store
    -retrieve
    -manage

Data Model:
    -collection of concepts for describing data.
    -Relational model of data: Most widely used model. Relation is shown through the table with rows and columns.

Schema:
    -description of a particular collection of data, using a given data model. Specifies name of relation, plus name and type of each column.

Relation(or table):
    -contains tuples and attributes. All rows are distinct.
    -#Rows = cardinality, #Fields = degree/arity.

Row(or tuple):
    -a set of fields that generally represents an "object" like a person or a music track.

Column(also attribute or field):
    -one of possibly many elements of data corresponding to the object represented by the row.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Database Design:
    -Phase 1: Requirement Analysis - What users(apps) expect from the database.
    -Phase 2: Conceptual Database Design - Build ER(Entity-Relationship) diagram.
    -Phase 3: Logical Database Design - Convert ER design into a relational database schema.

    -Conceptual design follows requirements analysis, yields a high-level description of data to be stored.
    -ER model is popular for conceptual design. Constructs are expressive, close to the way people think about their applications.
    -Basic constructs: entities, relationships, and attributes (of entities and relationships).
    -Some additional constructs: weak entities, ISA hierarchies, and aggregation.
    -Several kinds of integrity constraints can be expressed in the ER model(Key, Participation, Overlap, Covering).

ER Model:
    -Entity:     real-world object distinguishable from other objects. An entity is described using a set of attributes.
    -Entity Set: a collection of similar entities. E.g., all employees.
                 1.All entities in an entity set have the same set of attributes.
                 2.Each entity set has a key.
                 3.Each attribute has a domain.
    -Relationship: association among 2 or more entities. A relationship is identified by its participating entities.
    -Relationship Set: a collection of similar relationships.
    -Key constraint: 1-to-1, 1-to-Many, Many-to-Many.
    -Participation Constraint: total vs partial(default).
    -Weak entity: can be identified uniquely only by considering the primary key of another(owner) entity.
                  1.Owner entity set and week entity set must participate in a one-to-many relationship set(one owner, many weak entities).
                  2.Weak entity set must have total participation in this identifying relationship set.
    -Class Hierarchies: as in C++, or other PLs, attributes are inherited. If we declare A ISA B, every A entity is also considered to be a B entity.
                        Overlap constraints: Can Joe be an Hourly_Emps as well as a Contract_Emps entity? (Allowed/Disallowed)
                        Covering constraints: Does every Employees entity also have to be an Hourly_Emps or a Contract_Emps entity? (Yes/No)
                        Reasons for using ISA: 1.To add descriptive attributes specific to a subclass. 2.To identify that participate in a relationship.
    -Aggregation: used when we have to model a relationship involving (entity sets and) a relationship set. Aggregation allows us to treat a relationship set as an entity
                  set for purposes of participation in (other) relationships.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Integrity Constraints: a condition that must be true for any instance of the database.
                       -ICs are specified when the schema is defined.
                       -ICs are checked when relations are modified.
                       -A legal instance of a relation is one that satisfied all specified ICs.
                       -DBMS should not allow illegal instances.
                       -If the DBMS check ICs, stored data is more faithful to real-world meaning.
                       -Avoids data entry errors, too!

Constructing the tables outwardly in.

Primary Key:
    -is used to ensure data in the specific column is unique. It is a column that cannot have NULL values. It is either an existing table column or a column that is specifically
     generated by the database according to a defined sequence.

Foreign Key:
    -is a column or group of columns in a relational database table that provides a link between data in two tables. It is a column(or columns) that references a column
     (most often the primary key) of another table. If all foreign key constraints are enforced, referential integrity is achieved, i.e., no dangling references.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Page access cost is usually the dominant cost of database operations.

Alternative File Organizationsï¼š
    Heap Files:
        -Random order.
        -Suitable when typical access is a file scan retrieving all records.
        -Advantages:
            Efficient -for bulk loading data.
                      -for relatively small relations as indexing overheads are avoided.
                      -when queries that need to fetch a large proportion of stored records.
        -Disadvantages:
            Not Efficient -for selective queries.
                          -for sorting, may be time-consuming.

    Sorted Files:
        -Best if records must be retrieved in some order, or only a range of records is needed.

    Indexes:
        -Search key + Data Reference
        -Data structures to organize records via trees or hashing.
        -Speeds up selections on the search key fields.
        -Any subset of the fields of a relation can be the search key for an index on the relation.
        -An index contains a collection of data entries and supports efficient retrieval of all data entries k* with a given key value k.
        -A file can be clustered on at most one search key.
        -Data entries contain the addresses of records.

     1.Hash-Based Indexes:
         -Good for equality selections.
         -Hashing function h: h(r) = bucket in which (data entry for) record r belongs. h looks at the search key fields of r.

     2.B+ Tree
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Transaction:
    -is the DBMS's abstract view of a user's program: a sequence of reads and writes. It is a collection of actions that make consistent transformations of system states
     while preserving system consistency.

ATOMICITY all or nothing:
    -A transaction might commit after completing all its actions, or it could abort after executing some actions.
    -Always executing all actions in one step, or not executing any actions at all.
    -DBMS logs all actions so that it can undo the actions of aborted transactions.

CONSISTENCY no violation of integrity constraints:
    -A transaction that executes alone against a consistent database leaves it in a consistent state.
    -Transactions do not violate database integrity constraints.
    -Transactions are correct programs.

ISOLATION concurrent changes invisible->serializable:
    -Serializability:    If several transactions are executed concurrently, the results must be the same as if they were executed serially in some order.
    -Incomplete results: An incomplete transaction cannot reveal its results to other transactions before its commitment.

DURABILITY committed updates to persist:
    -Once a transaction commits, the system must guarantee that the results of its operations will never be lost, in spite of subsequent failures.
    -Database recovery.

Scheduling Transactions:
    -1.Serial Schedule:       Schedule that does not interleave the actions of different transactions.
    -2.Equivalent Schedule:   For any database state, the effect of executing the first schedule is identical to the effect of executing the second schedule.
    -3.Serializable Schedule: A schedule that is equivalent to some serial execution of the transactions.

Conflict Serializable Schedules:
    -Two schedules are conflict equivalent if: 1.Involve the same actions of the same transactions.
                                               2.Every pair of conflicting actions is ordered the same way.
Schedule S is conflict serializable if S is conflict equivalent to some serial schedule.

Strict Two-Phase Locking (Strict 2PL) Protocol:
    -Each transaction must obtain a shared lock on object before reading, and an exclusive lock on object before writing.
    -All locks held by a transaction are released when the transaction completes.
    -Strict 2PL allows only serializable schedules.
    -It simplifies transaction aborts.

Lock manager:
    -Lock and unlock requests are handled by the lock manager.
    -Locking and unlocking have to be atomic operations.
    -Lock upgrade: transaction that holds a shared lock can be upgraded to hold an exclusive lock.

Deadlock:
    -Cycle of transactions waiting for locks to be released by each other.
    -Two ways of dealing with deadlocks: 1.Deadlock Prevention 2.Deadlock Detection

Deadlock Detection:
    Create a waits-for graph:
        -Nodes are transactions.
        -There is an edge from Ti to Tj if Ti is waiting for Tj to release a lock.
        -Periodically check for cycles in the waits-for graph.

Fault->(causes)Error->(results in )Failure

Database Log:
    -Every action if a transaction must not only perform the action, but must also write a log record to an append-only file.
    -The following actions are recorded in the log:
        Ti writes an object: the old value and the new value. Log record must go to disk before the changed page.
        Ti commits/aborts: a lot of record indicating this action.
    -Log records are chained together by transactions id, so it's easy to undo a specific transaction.
    -All log related activities are handled transparently by the DBMS.

Logging:
    -The log contains information used by the recovery process to restore the consistency of a system. This information may include:
        1.transaction identifier
        2.type of operation(action)
        3.items accessed by the transaction to perform the action
        4.old value(state) of item(before image)
        5.new value(state) of item(after image)

Write-Ahead Log Protocol:
    -If a system crashes before a transaction is committed, then all the operations must be undone. Only need the before images(undo).
    -Once a transaction is committed, some of its actions might have to be redone. Need the after images(redo).
    -Before a stable database is updated, the undo portion of the log should be written to the stable log.
    -When a transaction commits, the redo portion of the log must be written to stable log prior to the updating of the stable database.
There are 3 phases in the ARIES(Algorithm for Recovery and Isolation Exploiting Semantics):
1.Analysis: Scan the log forward(from the most recent checkpoint) to identify all transactions that were active, and all dirty pages in the buffer pool at the time of the crash.
2.Redo
3.Undo

Concurrency control and recovery are among the most important functions provided by a DBMS.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Distribution: Sharing information and services: 1.Availability 2.Scalability(volume and the number of users served) 3.Reliability/Fault Tolerance 4.Transparency

Distributed Database System:
    -A collection of multiple, logically interrelated databases distributed over a computer network.
    -The software that manages the DDB and provides an access mechanism that makes this distribution transparent to the users.
    -Distributed DBMS Promises:
        1.Transparent management of distributed, fragmented, and replicated data.
        2.Improved reliability/availability through distributed transactions.
        3.Improved performance.
        4.Easier and more economical system expansion.

Transparency: Separation of the higher level semantics of a system from the lower level implementation issues.
              Fundamental issue is to provide data independence in the distributed environment.

Fragmentation Alternatives: 1.Horizontal 2.Vertical
              Properties:
                  -Completeness: Decomposition of relation R into fragments R1, R2, ..., Rn is complete if and only if each data item in R can also be found in some Ri.
                  -Reconstruction: If relation R is decomposed into fragments R1, R2, ..., Rn, then there should exist some relational operator âˆ‡ such that R = âˆ‡1â‰¤iâ‰¤nRi.
                  -Disjointness: If relation R is decomposed into fragments R1, R2, ..., Rn, and data item di is in Rj, then di should not be in any other fragment Rk (kâ‰ j).
              Problem Statement: F={F1, F2, â€¦, Fn} fragments, S={S1, S2, â€¦, Sm} network sites, Q={q1, q2,â€¦, qq} applications. Find the "optimal" distribution of F to S.
              Optimality: 1.Minimal cost 2.Performance(Minimal query response time or maximum throughput).

Replication: 1.Increased availability 2.Faster query evaluation 3.Updates are challenging
             Potentially improved performance: 1.proximity of data to its points of use 2.requires some support for fragmentation and replication
             Allocation Alternatives:
                 -Non-replicated: partitioned: each fragment resides at only one site.
                 -Replicated: 1.fully replicated: each fragment at each site.
                              2.partially replicated: each fragment at some of the sites(most of the databases these days).
             Rule of Thumb: if (read-only queries/update queries) >> 1, replication is advantageous, otherwise replication may cause problems.

Query Processing in a DDBMS: High level user query -> Query Processor -> Low-level data manipulation commands for D-DBMS
                             Query language that is used: SQL
                             Query execution methodology: The steps that one goes through in executing high-level(declarative) user queries.
                             Query Processing:
                                 Decomposition: Same as in centralized system. 1.Normalization(Convert from general language to a "standard" form(e.g., Relational Algebra))
                                                                               2.Eliminating redundancy
                                                                               3.Algebraic rewriting: After decomposition: One or more algebraic query trees on relations.
                                                                                                      Localization: Replace relations by corresponding fragments.
                                 Localization: 1.Start with query 2.Replace relations by fragments 3.Push âˆª up or Ï€,Ïƒ down 4.Simplify: eliminate unnecessary operations
                                 Optimization:
                                     Solution Space: The set of equivalent algebra expressions(query trees).
                                     Cost Function(in terms of time):  1.I/O cost+CPU cost+communication cost.
                                                                       2.These might have different weights in different distributed environments(LAN vs WAN).
                                                                       3.Can also maximize throughput.
                                     Search algorithm: 1.How do we move inside the solution space?
                                                       2.Exhaustive search, heuristic algorithms (iterative improvement, simulated annealing, genetic,â€¦)
                                 Optimization Statistics: Primary cost factor: size of intermediate relations(Need to estimate their sizes).
                                                          Make them precise more costly to maintain.
                                                          Simplifying assumption: uniform distribution of attribute values in a relation.
                                 Query Separation: Separate query into 2 or more steps. Optimize each step independently.
                                 Decompose query intoï¼šLocal processing -> Simple query(or queries) -> Final processing.
                                 Simple Query: Relations have a single attribute. Output has a single attribute.

A distributed database system does not need to be a parallel database system, but it can be.

Parallel DBMS:
    -Pipeline  Parallelism: many machines each doing one step in a multi-step process.
     Partition Parallelism: many machines doing the same thing to different pieces of data.
    -Speed-Up: More resources means proportionally less time for given amount of data.
     Scale-Up: If resources increased in proportion to increase in data size, time is constant.

Parallel Database Architectures:
    -Shared Memory(SMP)-centralized: 1.Easy to program 2.Expensive to build 3.Difficult to scaleup
    -Shared Disk-centralized
    -Shared Nothing(network): 1.Hard to program 2.Cheap to build 3.Easy to scaleup

Different Types of DBMS parallelism(query processing in a database happens by running operators on the data):
    -Intra-operator parallelism: get all machines working to compute a given operation(Operators: scan, sort, join).
    -Inter-operator parallelism: each operator may run concurrently on a different site(exploits pipelining).
    -Inter-query parallelism: different queries run on different sites.

Data Partition(partitioning a table):
    -Range
    -Hash
    -Round Robin

Partial Scans: Scan in parallel, and merge. Selection may not require all sites for range or hash partitioning. Indexes can be built at each partition.
Parallel Sorting:
    -Scan in parallel, and range partition as you go.
    -As tuples come in, begin "local" sorting on each.
    -Resulting data is sorted, and range-partitioned.
    -Problem: skew.
    -Solution: "sample" the data at start to determine partition positions.
Parallel Joins:
    -Nested loop: Each outer tuple must be compared with each inner tuple that might join. Easy for range partitioning on join columns, hard otherwise.
    -Sort-Merge(or plain MergeJoin): Sorting gives range partitioning. Merging partitioned tables is local.

Scaling Up(or vertical scaling): make a "single" machine more powerful -> dataset is just too big.

Scaling Out(or horizontal scaling): adding more smaller/cheaper servers is a better choice.
    -Different approaches for horizontal scaling(multi-node database):
         Master(Writes to only one machine)/Slave(Reads)
         Sharding(partitioning): Scales well for both reads and writes. Distributed Query Processing is challenging.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
NoSQL:
    -began in 1998
    -Disadvantages: 1.Does not fully support relational features: No join, group by, order by operations. No referential integrity constraints across partitions.
                    2.No declarative query language.
                    3.Relaxed ACID -> fewer guarantees.
                    4.No easy integration with other applications that support SQL.
    -Requirements: 1.Availability 2.Scalability 3.Fault Tolerance 4.Manageability

CAP Theorem(2 out of 3):
    -Consistency: All copies have the same value. A consistency model determines rules for visibility and apparent order of updates. ACID is hard to achieve, moreover, it is
                  not always required.
         Strong consistency â€“ ACID(Atomicity, Consistency, Isolation, Durability)
         Weak consistency   â€“ BASE(Basically Available Soft-state Eventual consistency)
         Eventual consistency: When no updates occur for a long period of time, eventually all updates will propagate through the system and all the nodes will be consistent.
    -Availability: Reads and writes always succeed.
    -Partition Tolerance: System properties hold even through failures. The system can allow partitioning across the cluster.

NoSQL Categories:
1.Key-value:
    Amazon Dynamo: Query Model: Simple R/W operations to data with unique IDs.
                                No operation span multiple records.
                                Data stored as binary objects of small sizes.
                   ACID Properties: Weaker(eventual) consistency.
                   Efficiency: Optimize for the 99.9%.
                   Service Level Agreements: Cloud-computing and virtual hosting contracts include SLAs. Most are described in terms of mean, median, and variance of response
                                             times(suffer from outliers).
                   Design Decisions: Incremental Scalability: must be able to add nodes on demand with minimal impact.
                                     Load Balancing
                                     Data Partitioning: Use consistent hashing - Each node gets an ID from the space of keys. Nodes are arranged in a ring. Data stored on the
                                                        first node clockwise of the current placement of the data key.
                                     Replication: Dynamo is an "always writable" data store. Preference lists of N nodes following the associated node.
                   System Interface: put(key, context, object)
                                         -key: primary key associated with data object.
                                         -context: vector clocks and history(needed for merging).
                                         -object: data to store.
                                     get(key)
                   Data Versioning: Updates generate a new timestamp.
                                    Eventual consistency - Multiple versions of the same object might co-exist.
                                    Syntactic Reconciliation - System might be able to resolve conflicts automatically.
                                    Semantic  Reconciliation - Conflict resolution pushed to application.
    Pros: very fast, very scalable, simple model, able to distribute horizontally.
    Cons: many data structures(objects) can't be easily modeled as key-value pairs.

2.Document-based(the documents that are exchanged across the Internet, different websites, web applications or mobile applications):
    The document is a unit that cannot be partitioned.
    JSON(unstructured or semi-structure or loose structure) Sample:
                       { _id : ObjectId("4c4ba5c0672c685e5e8aabf3"),
                             author : "roger",
                             date : "Sat Jul 24 2010 19:47:11 GMT-0700 (PDT)",
                             text : "MongoSF",
                             tags : [ "San Francisco", "MongoDB" ] }
                         _id is unique, but can be anything you'd like.
    JSON has powerful, but limited set of datatypes.

    MongoDB:
        -extends datatypes with Date, Int types, Id...
        -stores data in BSON, BSON is a binary representation of JSON. Optimized for performance and navigational abilities. Also for compression purposes.
        -See bsonspec.org
    Unsharded Deployment:
        -configure as a replica set for automated failover.
        -async replication between nodes.
        -add more secondaries to scale reads.
    Sharded Deployment:
        -autosharding distributes data among two or more replica sets.
        -Mongo Config Server(s) handles distribution and balancing.
        -transparent to applications.
    -Advantages:
         Documents are independent units: make performance better(related data is read contiguously off disk) and make it easier to distribute data across multiple servers
                                          while preserving its locality.
         Application logic is easier to write: don't have to translate between objects in your application and SQL queries. Just turn the object model directly into a document.
         Unstructured data can be stored easily: a document contains whatever keys and values the application logic requires.
    -Disadvantages:
         Cannot do a join.
         No referential integrity.
         Not easy to integrate SQL application with document store.

3.Column-based(kind of an extension to the key-value):
    Google Big Table - build a more application-friendly storage service using these parts:
        -1.Scheduler(Google WorkQueue)
        -2.Google Filesystem
        -3.Chubby Lock service: {lock/file/name} service. Coarse-grained locks, can store small amounts of data in a lock.
        -4.Other pieces helpful but not required: MapReduce
        Data Mode:
            <Row, Column, Timestamp> triple for key - lookup, insert, and delete.
            Arbitrary "columns" on a row-by-row basis: Column family: qualifier. Family is heavyweight, qualifier is lightweight. Column-oriented physical store-rows are sparse.
            Does not support a relational model: No table-wide integrity constraints. No multi-row transactions.
            SSTable: Immutable, sorted file of key-value pairs.
                     Chunks of data plus an index: Index is of block ranges, it's not value.
            Tablet: Contains some range of rows of the table. Built out of multiple SSTables.
            Table: Multiple tablets make up the table. SSTables can be shared. Tablets do not overlap, SSTables can overlap.
        Servers:
            Tablet servers manage tablets, multiple tablets per server. Each tablet lives at only one server. Tablet server splits tablets that get too big.
            Master responsible for load balancing and fault tolerance. GFS replicates data. Prefer to start tablet server on same machine that the data is already.

4.Graph-based(nodes and edges or vertices and edges):
    -Social Graph, Road Network Graph, Brain Graph, etc.
    -Graph Query(sequence of node and edge predicates) is a regular expression: The graph query basically it's either return a path in the graph or return a pattern in the graph
     based on some specifications that the user say in the application.
    -Execution: Breadth First Search.
    -The algebraic plan basically it's a finite-state machine on the graph query, that represent the graph query and it runs a bunch of selection operators. Selection operators
     mean that you filter the nodes or the edges based on the constraints.
    -Plan Enumeration for Query Optimization: Apply dynamic programming - complexity: O(n^3).
     Algebraic Operators:
         1.Select:           Find set of starting nodes.
         2.Traverse(Expand): Traverse graph to construct paths.
         3.Join:             Construct longer paths.

Schema-Less(Document-based, Column-based, Graph-based):
    -Pros: Schema-less data model is richer than key-value pairs: eventual consistency, many are distributed, still provide excellent performance and scalability.
    -Cons: typically no ACID transactions or joins.
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
MapReduce System:
    -Executes user's jobs specified as "map" and "reduce" functions.
    -Manages work distribution and fault-tolerance.
     If a task crashes: Retry on another node. OK for a map because it had no dependencies. OK for reduce because map outputs are on disk.
     If the same task repeatedly fails, fail the job.
     If a node crashes: relaunch its current tasks on other nodes.
                        relaunch any map the node previously ran(necessary because their output files were lost along with the crashed node).
    -Master node: one machine that has process called the master process, and it keeps track of the MapReduce program written by the user. And it also keeps track of the
                  cluster, how to settle the cluster, the addresses and locations of the machines. And it decides which machine runs what, at the moment, for each phase.
     Map function: each one of them will run the same exact code defined by the application programmer. After the map phase, every map worker stores the results of the map
                   phase on local disk before serving to reducer. Allows having more reducers than nodes. Allows recovery if a reducer crashes. Preferentially placed on same
                   node or same rack as their input block. Push computation to data, minimize network use.
     Data shuffle phase: or the data transfer phase. A hidden phase that is done internally by the run time. The application programmer does not see it, does not control it.
                         It's automatically happening between the map and the reduce phase.
     Reduce function
    -Programming Modelï¼šData type: key-value records.
                        Map function: (Key1, Value1) -> list(Key2, Value2)
                        Reduce function: (Key2, list(Value2)) -> list(Key3, Value3)

Distributed file system(HDFS):
    -Single namespace for the entire cluster.
    -Replicates data 3x for fault tolerance.
    -HDFS Files are split into 128MB blocks.
    -Blocks replicated across several DataNodes(the nodes that contain the data).
    -NameNode(centralized) is a node or a machine that stores metadata about the blocks.
    -Optimized for large files, sequential reads.
    -Files are append-only(can only append content or data to the file and you can not change the file, can not go back or update the file).

Hadoop is an implementation of the MapReduce paradigm, it relies on the Hadoop File System. In Hadoop, you write the map and the reduce functions using Java code. And you build
a jar file.

Apache Spark:
    -a cluster computer system, a shared-nothing architecture.
    -rely on Resilient Distributed Dataset(RDD).

Apache Hadoop Ecosystem: Two main components: MapReduce + Hadoop file system
Apache Hive:      SQL-like(HiveQL) data warehouse on top of Hadoop.
Apache Pig:       process data flow programs(Pig Latin Language) on top of Hadoop.
Apache Hbase:     NoSQL distributed database(based on BigTable) on top of HDFS.
Apache ZooKeeper: distributed configuration service, synchronization service, and registry for Hadoop.
Apache Mahout:    designed on top of Hadoop or MapReduce and HDFS to provide out-of-the-box MapReduce implementation of machine learning algorithms.

Convert Spatial Data to 1D data: space-filling curve
Spatial partitioning:
    -Uniform grid: not very scalable, not very efficient, may lead to data skew(load balancing problem). Can be good for uniformly distributed data.
    -KDB-Tree:     it just partitions the space in the sense that there is no overlap. It assigns more partitions for the more dense area, less for the sparse areas.
    -Quad-Tree:    assigns partitions based on how dense and starts partitioning the data into four quadrants as you go down.
    -R-Tree:       partition that data and finds minimum bounding rectangles for data that is clustered in the same location, and the index entries are actually these minimum
                   bounding rectangles.
    -Voronoi Diagram
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Query Language:
Procedural/Imperative Language: Relational Algebra
    -Instructs the system to perform a sequence of operators to compute a result.
Non-Procedural/Declarative Language: Relational Calculus, SQL
    -Tells what data is to be retrieved but does not tell the system how to retrieve the data.

SQL(Structured Query Language):
    -is the language we use to issue commands to the database.
    -CRUD(create, read, update, delete)

SQL support all 4 options on deletes and updates:
    -Default is NO ACTION (delete/update is rejected).
    -CASCADE (also delete all tuples that refer to deleted tuple).
    -SET NULL/SET DEFAULT (sets foreign key value of referencing tuple).

SELECT [DISTINCT] target-list
FROM relation-list
WHERE qualification
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
cursor:
    -is a control structure that enables traversal over the records in a database. Cursors facilitate subsequent processing in conjunction with the traversal, such as retrieval,
     addition and removal of database records. The database cursor characteristic of traversal makes cursors akin to the programming language concept of iterator.

PostgreSQL command:
psql -U postgres (the password is the one set during the installation)
\c database_name; - Connect to a specific database.
\dt               - Lists all tables in a current database.
\q                - To quit the psql.

psycopg:
    -Psycopg is the most popular PostgreSQL database adapter for the Python programming language. Its main features are the complete implementation of the Python DB API 2.0
     specification and the thread safety (several threads can share the same connection). It was designed for heavily multi-threaded applications that create and destroy lots
     of cursors and make a large number of concurrent INSERTs or UPDATEs.

    connect():
        -To connect to the database, you use the connect() function of the psycopg2 module.
        -The connect() function creates a new database session and returns a new instance of the connection class. By using the connection object, you can create a new cursor to
         execute any SQL statements.
        -To call the connect() function, you specify the PostgreSQL database parameters as a connection string and pass it to the function like this:
         conn = psycopg2.connect("dbname=name user=postgres password=postgres")
        -conn.commit() make the changes to the database persistent.

    cursor = connection.cursor():
        -Using connection.cursor() we can create a cursor object which allows us to execute PostgreSQL command through Python source code. We can create as many cursors as we
         want from a single connection object. Cursors created from the same connection are not isolated, i.e., any changes done to the database by a cursor are immediately
         visible by the other cursors. Cursors are not thread-safe.
        -After this, we printed PostgreSQL Connection properties using a connection.get_dsn_parameters().

    cursor.execute():
        -Using the cursor's execute method we can execute a database operation or query. Execute method takes a SQL query as a parameter. We can retrieve query result using
         cursor methods such as fetchone(), fetchmany(), fetcthall().

    cursor.close() and connection.close():
        -It is always good practice to close the cursor and connection object once your work gets completed to avoid database issues.
